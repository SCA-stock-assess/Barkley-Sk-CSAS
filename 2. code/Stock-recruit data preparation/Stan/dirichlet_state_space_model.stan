// Code generated by ChatGPT based on description of the problem and 
// uploading the raw data file


data {
  int<lower=1> T;                       // Number of time points
  int<lower=2> K;                       // Number of composition categories
  vector<lower=0>[K] rate[T];           // Weight data (non-integer) for Dirichlet likelihood
  int<lower=0, upper=1> is_observed[T]; // Indicator for observed values
  simplex[K] prior_mean;                 // Informative prior on initial composition (e.g., historic average)
  vector<lower=0>[T] conc;               // Year-specific concentration parameter (weights/sample sizes)
  real<lower=0> sigma_prior;             // Lake-specific prior for sigma
}

parameters {
  simplex[K] theta[T]; // Raw latent compositions for non-centered parameterization
  real<lower=0> sigma;     // Process noise
  vector[T-1] epsilon_raw;  // Standard normal noise for process variation
}

transformed parameters {
  vector[T-1] epsilon;
  epsilon = sigma * epsilon_raw; // Scale standard normal noise
}

model {
  sigma ~ normal(sigma_prior, 0.5) T[0,]; // Lake-specific prior for sigma
  // First value (user-specified) is the mean, second value (0.5) is the SD.
  epsilon_raw ~ normal(0, 1);             // Standard normal prior
  theta[1] ~ dirichlet(prior_mean * 200); // Prior informed by historic average
    // using 200 to reflect reasonable confidence that the historic average should
  // be representative of the population under status quo conditions

  for (t in 2:T) {
    vector[K] epsilon_t;
    for (k in 1:K) {
      epsilon_t[k] = exp(epsilon[t-1]); // Small perturbation
    }
    theta[t] ~ dirichlet(theta[t-1] .* epsilon_t + 1e-6); // Process noise added
  }

  for (t in 1:T) {
    if (is_observed[t] == 1) {
      rate[t] ~ dirichlet(theta[t] * conc[t]); // Year-specific concentration parameter
    }
  }
}

